■ 텐서 플로우란 ? 

  텐서 플로우(TensorFlow) 는 기계학습과 딥러닝을 위해 구글에서 만든 오픈 소스 라이브러리이다. 








■ 텐서        플로우의 장점 
    ↓           ↓
  다차원 배열의 흐름 (4차원 배열의 연산(계산)을 빠르게 할 수 있게금 구현이되어짐)
	  1. 코드가 간결해진다.  
	  2. 신경망 구현에 필요한 모든 함수들이 다 내장 되어있다.
	  3. 속도가 빠르다. ( 코딩도 빨라지고 실행 빠르다)
	  4. GPU 를 사용할 수 있다. 

	한국시간으로 2016년 11월 29에 TensorFlow v0.12.0 RC0 이 업데이트 되었고 2016년 11월 29일 나온 버젼의 
	핵심  변경사항은 window 에서 GPU 버젼의 텐써 플로우를 지원한다는 것이었다. 예전에는 Ubuntu 에서만 
	가능하던 GPU 버젼도 윈도우에서 설치가 가능하게 되었다. 









■ 텐써 플로우 코드의 구조 

	      모델을 생성하는 부분
	             - 오퍼레이션
	             - 변수 
	   -----------------------------
	  
	     모델을 실행하는 부분 
	             - 세션 








■ 텐서 플로우 용어 설명 

	1. 오퍼레이션 (Operation)

		그래프 상의 노드는 오퍼레이션(줄임말 op) 로 불린다. 오퍼레이션은 하나 이상의 텐서를 받을 수 
		있다. 오퍼레이션은 계산을 수행하고, 결과를 하나 이상의 텐서로 반환 할 수 있다.


	2. 텐써 (Tensor)

		내부적으로 모든 데이터는 텐써를 통해 표현된다. 텐써는 일종의 다차원 배열인데, 그래프 
		내의 오퍼레이션간에 텐써가 전달 된다.



	3. 세션 (Session)

		그래프를 실행하기 위해서는 세션 객체가 필요하다. 세션은 오퍼레이션의 실행환경을 캡슐화 
		한것이다. 


	      모델을 생성하는 부분  <-- 그래프를 그리는 부분
	             - 오퍼레이션
	             - 변수 
	   -----------------------------
	  
	     모델을 실행하는 부분  <--  만들어진 그래프에 데이터를
	             - 세션             주입하는 부분 


	4. 변수 (Variable)

		 변수는 그래프의 실행시, 파라미터를 저장하고 갱신하는데 사용된다. 
		 메모리상에서 텐서를 저장하는 버퍼 역활을 한다. 






■ 텐써 책 목차

 1. 텐써 플로우 설치
 2. 텐써 플로우 기본 문법 
 3. 단일 계층 신경망
 4. 다중 계층 신경망 
 5. Vgg, Inception ...






■ 신경망에 입력할 데이터들

	  1. mnist
	      ↓
	  2. cifar10
	      ↓
	  3. 개/고양이
	      ↓
	  4. 정상폐/폐결절
	      ↓
	  5. 이파리 사진 ( 상품의 표지의 기스 여부 확인)

	예제1.  
	
	import  tensorflow  as  tf
	
	sess = tf.Session()  # 그래프를 실행할 세션을 구성한다. 
	
	hello = tf.constant('Hello, Tensorflow')

		변수를 정의하는 영역 
		-----------------------------------------------------------
		변수를 실행하는 영역 
		위에서 변수를 정의했지만, 실행은 정의한 시점에서 실행되는
		것은 아니다. 

	session 객체와 run 메소드를 사용할때 계산이 되어 실행된다.
	
	print ( sess.run(hello)) 
	print (str(sess.run(hello), encoding="utf-8")
	
	b'Hello , Tensorflow'
	
	Hello, Tensorflow 
	
		※ 파이썬 3버젼은 문자열 unicode 가 기본이므로 str 에서 encoding 처리를 해줘야
		   binary 타입을 unicode 타입으로 반환한다.









■ 텐써 플루우 기본 실습 두번째 예제

	import  tensorflow  as  tf # tensorflow 모듈을 가져와서 tf 호출

	x = tf.constant(35, name='x') # x라는 상수값을 만들고 
	                              # 숫자 35를 지정 
	y = tf.Variable( x + 5, name='y') # y 라는 변수를 만들고
	                                  # 방정식 x+5 로 정의함 
	model = tf.global_variables_initializer()

	# global_variables_initializer() 로 변수를 초기화 하겠다.

	# 그래프를 그리는 영역(빌딩 building 영역)
	# -------------------------------------------------
	# 그래프를 실행하는 영역 

	sess = tf.Session()  # 그래프를 실행할 세션을 구성한다. 
	sess.run(model) # 변수를 초기화 하겠다고 정의한 model 를
	                # 실행하겠다. 

	print (sess.run(y)) 

	40



■ 텐써 플로우 세번째 예제

	import  tensorflow  as  tf
	
	a = tf.constant(10)
	b = tf.constant(32)
	c = tf.add(a,b)
	print(c)

	Tensor("Add_1:0", shape=(), dtype=int32)	





문제1. 위에서 만든 텐써 그래프를 실행하시오 !

	sess = tf.Session()
	
	print (sess.run(c))





문제2. 아래의 모델(그래프) 를 실행하시오 !

	import  tensorflow  as  tf
	
	a = tf.add(1,2)
	b = tf.multiply(a,3)
	c = tf.add(b,5)
	d = tf.multiply(c,6)
	e = tf.multiply(d,5)
	f = tf.div(e,6)
	g = tf.add(f,d)
	h = tf.multiply(g,f)
	
	sess = tf.Session()
	print (sess.run(h)) 
	
	10780





문제3. 위의 예제를 with 절 사용해서 구현하시오 !

	import  tensorflow  as  tf
	
	a = tf.add(1,2)
	b = tf.multiply(a,3)
	c = tf.add(b,5)
	d = tf.multiply(c,6)
	e = tf.multiply(d,5)
	f = tf.div(e,6)
	g = tf.add(f,d)
	h = tf.multiply(g,f)
	
	sess = tf.Session()
	print (sess.run(h)) 
	sess.close()  <---  세션을 닫는 구문을 작성해줘야한다. 

	답:
	
	with  tf.Session()  as  sess:
	    print ( sess.run(h) )
	
		※ with 절을 사용하면 close() 를 안써도 된다. 






■ 파이썬 기본문법과 텐써 플로우 기본 문법을 비교 

	 1 에서 5까지의 숫자를 출력한다. 
	
	- 파이썬 기본 문법
	
		x = 0
		for i in range(5):
		    x = x + 1
		    print (x)
	
	- 텐써 플로우 
	
		import  tensorflow  as  tf
		
		x = tf.Variable(0)
		
		model = tf.global_variables_initializer()
		# 이전에 생성했던 변수를 초기화 하는게 아니라 
		# 변수를 생성했으면 무조건 초기화를 해줘야 실행이 된다. 
		
		sess = tf.Session()
		sess.run(model)
		
		for i in range(5):
		    x = x + 1
		    print (sess.run(x)) 
		
		sess.close()






문제4. 위의 코드를 이용해서 구구단 2단을 출력하시오 !

	 tf.multiply 를 활용하세요 !
	
	 2 x 1 = 2
	 2 x 2 = 4
	 2 x 3 = 6
	    :
	 2 x 9 = 18 
	
	답:
	
	import  tensorflow  as  tf
	
	x = tf.Variable(2)
	y = tf.Variable(1)
	
	model = tf.global_variables_initializer()
	
	sess = tf.Session()
	
	sess.run(model)
	
	for i in  range(9):
	    z = tf.multiply(x,y)
	    print ( sess.run(x)," x ", sess.run(y), " = " , sess.run(z))
	    y = y + 1
	
	-- 준하 코드
	
	import tensorflow as tf
	
	sess = tf.Session()
	for i in range(1,10):
	    y = i
	    x = tf.multiply(2, y, name='x')
	    print('2 *',i,'='+str(sess.run(x)))
	
	sess.close()






문제5. 구구단 2단에서 9단까지 출력하시오 ! 

	import  tensorflow  as  tf
	
	x = tf.Variable(0)
	y = tf.Variable(0)
	z = tf.multiply(x,y)
	
	model = tf.global_variables_initializer()
	
	sess = tf.Session()
	
	sess.run(model)
	
	for i in  range(2,10):
	    for j in  range(1,10):
	        print (i," x ", j," = ",sess.run(z,feed_dict={x:i,y:j}))
  






문제6.(점심시간 문제) 아래의 텐써 그래프를 실행하시오 !
       숫자는 알아서 feed 하세요 ~

	import  tensorflow  as  tf
	
	a = tf.placeholder("float")
	b = tf.placeholder("float")
	
	y = tf.multiply(a,b)
	z = tf.add(y,y)









■ numpy와 tensorflow문법 비교



문제 7. zero와 숫자 1을 채워넣는 배열을 생성하시오 !

	1. numpy
		import numpy as np
		
		a = np.zeros((2, 2))
		b = np.ones((2, 2))
		print(a)
		print(b)

		[[0. 0.]
		 [0. 0.]]
		[[1. 1.]
		 [1. 1.]]


	2. tensorflow

		import tensorflow as tf
		
		a = tf.zeros((2, 2))
		b = tf.ones((2, 2))
		
		sess = tf.Session()
		print(sess.run(a))
		print(sess.run(b))
		
		sess.close()

		[[0. 0.]
		 [0. 0.]]
		[[1. 1.]
		 [1. 1.]]





문제 8. 아래의 numpy문법을 tensorflow로 구현하시오 !

	1. numpy
		import numpy as np
		
		a = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
		print(np.argmax(a, axis = 0))
	
		3
	
	
	2. tensorflow
		import numpy as np
		import tensorflow as tf
		
		a = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
		b = tf.argmax(a, axis=0)
		
		sess = tf.Session()
		print(sess.run(b))
		sess.close()
	
		3




문제 9. 아래의 numpy문법을 텐서 플로우로 구현하시오 !

	1. numpy
	
		import  tensorflow  as  tf
		import  numpy  as  np
		
		a = np.array([[[1, 2, 3],
		               [2, 1, 4],
		               [5, 2, 1],
		               [6, 3, 2]],
		              [[5, 1, 3],
		               [1, 3, 4],
		               [4, 2, 6],
		               [3, 9, 3]],
		              [[4, 5, 6],
		               [7, 4, 3],
		               [2, 1, 5],
		               [4, 3, 1]]])
		
		print  ( np.sum( a, axis = 0) )
	
		[[10  8 12]
		 [10  8 11]
		 [11  5 12]
		 [13 15  6]]
	
	
	2. tensorflow
	
		import  tensorflow  as  tf
		import  numpy  as  np
		
		a = np.array([[[1, 2, 3],
		               [2, 1, 4],
		               [5, 2, 1],
		               [6, 3, 2]],
		              [[5, 1, 3],
		               [1, 3, 4],
		               [4, 2, 6],
		               [3, 9, 3]],
		              [[4, 5, 6],
		               [7, 4, 3],
		               [2, 1, 5],
		               [4, 3, 1]]])
		b = tf.reduce_sum(a, reduction_indices = [0])
		
		sess = tf.Session()
		print(sess.run(b))
		sess.close()
		
		[[10  8 12]
		 [10  8 11]
		 [11  5 12]
		 [13 15  6]]




문제 10. 아래의 numnpy문법을 tensorflow로 구현하시오 !

	1. numpy
		import numpy as np
		
		a = np.array([i for i in range(144)])
		b = a.reshape(12,12)
		print(b.shape)
	
		(12, 12)
	
	2. tensorflow
		import tensorflow as tf
		import numpy as np
		
		a = np.array([i for i in range(144)])
		b = tf.reshape(a, (12, 12))
		
		sess = tf.Session()
		print(sess.run(b))
		print(b.get_shape())
		
		sess.close()
	
		[[  0   1   2   3   4   5   6   7   8   9  10  11]
		 [ 12  13  14  15  16  17  18  19  20  21  22  23]
		 [ 24  25  26  27  28  29  30  31  32  33  34  35]
		 [ 36  37  38  39  40  41  42  43  44  45  46  47]
		 [ 48  49  50  51  52  53  54  55  56  57  58  59]
		 [ 60  61  62  63  64  65  66  67  68  69  70  71]
		 [ 72  73  74  75  76  77  78  79  80  81  82  83]
		 [ 84  85  86  87  88  89  90  91  92  93  94  95]
		 [ 96  97  98  99 100 101 102 103 104 105 106 107]
		 [108 109 110 111 112 113 114 115 116 117 118 119]
		 [120 121 122 123 124 125 126 127 128 129 130 131]
		 [132 133 134 135 136 137 138 139 140 141 142 143]]
		(12, 12)



문제 11. (텐서 플로우로 구현한 단층 신경망 이해에 중요 문법) 아래의 numpy배열의 열단위 sum을 출력하시오 !

	1. numpy
		import numpy as np
		import tensorflow as tf
		
		x = np.arange(6).reshape(2, 3)
		print(np.sum(x, axis=0))
		
		[3 5 7]
	
	
	2. tensor
		import numpy as np
		import tensorflow as tf
		
		x = np.arange(6)
		y = tf.reshape(x, (2, 3))
		z = tf.reduce_sum(y, reduction_indices = [0])
		
		sess = tf.Session()
		print(sess.run(z))
		
		sess.close()
		
		[3 5 7]



문제 12. (텐서 플로우로 구현한 단층 신경망 이해에 중요 문법) 아래의 두 행렬의 합을 tensorflow로 구현하시오 !

	  0 0 0 + 1 1 1  =  1 1 1
	  0 0 0   1 1 1     1 1 1
	
	import tensorflow as tf
	
	a = tf.zeros([2, 3])
	b = tf.ones([2, 3])
	result = tf.add(a, b)
	
	sess = tf.Session()
	
	print(sess.run(a))
	print(sess.run(b))
	print(sess.run(result))
	sess.close()

	[[0. 0. 0.]
	 [0. 0. 0.]]
	[[1. 1. 1.]
	 [1. 1. 1.]]
	[[1. 1. 1.]
	 [1. 1. 1.]]






문제 13. 아래의 행렬의 내적을 tensoflow로 구현하시오 !

	 2 2 2  ◎  3 3      
	 2 2 2      3 3  =  ?
	            3 3 
	
	import tensorflow as tf
	import numpy as np
	
	x = tf.placeholder("float", [2, 3])
	y = tf.placeholder("float", [3, 2])
	result = tf.matmul(x, y)
	
	sess = tf.Session()
	
	print(sess.run(result, feed_dict = {x : [[2,2,2],[2,2,2,]],
	                                    y : [[3,3],[3,3],[3,3]]}))
	sess.close()

	[[18. 18.]
	 [18. 18.]]

	※ 설명 : x = tf.constant(10)			# 숫자 10 상수를 선언
		  x = tf.Variable(0)			# x 라는 변수를 만드는데 0 으로 값을 초기화
		  x = tf.placeholder("float")		# x 라는 실수형 데이터를 담을 변수만 선언
							# (값을 초기화하지 않았음)
		  x = tf.placeholder("float", [2, 3])	# x 라는 실수형 데이터를 행렬로 담을 변수를 선언




문제 14. (tensorflow의 cast함수의 이해) 아래의 배열의 True를 1로 변경하고 False를 0으로 변경시키시오 !

	import tensorflow as tf
	
	correct_prediction = [ True, False , True  ,True  ,True  ,True  ,True,  True  ,True  
	  ,True  ,True  ,True
	  ,True  ,True  ,True, False , True  ,True, False , True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True,
	  True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True ,False , True  ,True  ,True  ,True  ,True
	  ,True  ,True, False , True, False , True  ,True  ,True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True
	 ,False , True  ,True  ,True]
	
	sess = tf.Session()
	
	a = tf.cast(correct_prediction,"float")
	
	print(sess.run(a))
	sess.close()
	
	[1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.
	 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
	 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.
	 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
	 0. 1. 1. 1.]





문제 15. 위의 출력된 결과에서 전체 갯수 중에 1이 몇개나 되는지 정확도를 출력하시오 1
	 전체 다 더해서 전체 갯수로 나눈 값을 아래와 같이 출력하시오 !

	import tensorflow as tf
	
	correct_prediction = [ True, False , True  ,True  ,True  ,True  ,True,  True  ,True  
	  ,True  ,True  ,True
	  ,True  ,True  ,True, False , True  ,True, False , True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True,
	  True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True ,False , True  ,True  ,True  ,True  ,True
	  ,True  ,True, False , True, False , True  ,True  ,True  ,True  ,True  ,True  ,True
	  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True  ,True
	 ,False , True  ,True  ,True]
	
	sess = tf.Session()
	
	a = tf.cast(correct_prediction,"float")
	b = tf.reduce_mean(a)

	print(sess.run(b))
	sess.close()

	0.93










■ mnist데이터로 단층 신경망 구현하기



문제 16. Tensorflow에 기본적으로 내장되어 있는 mnist데이터를 가져오시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
	batch_xs, batch_ys = mnist.train.next_batch(100)
	
	print(batch_xs.shape)
	print(batch_ys.shape)
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 784)
	(100, 10)




문제 17. 위의 mnist 데이터 중에 train데이터의 라벨을 one hot encoding하지 말고 숫자로 100개의 라벨을 가져오시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/")
	batch_xs, batch_ys = mnist.train.next_batch(100)
	
	print(batch_xs.shape)
	print(batch_ys)
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 784)
	[4 3 2 5 0 2 0 2 8 8 5 3 1 6 0 6 9 9 6 5 0 2 8 9 5 3 3 1 5 9 8 8 3 0 6 5 9
	 8 4 2 3 2 8 9 1 7 2 8 5 1 4 2 5 4 6 0 9 2 0 3 2 7 8 3 6 5 6 5 0 3 7 7 4 1
	 2 7 8 4 4 9 1 8 5 3 5 8 4 0 5 7 4 7 1 9 1 9 9 3 7 0]





문제 18. 이번에는 test데이터와 test 데이터의 라벨을 100개를 가져오는데 shape만 출력하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	batch_xs, batch_ys = mnist.test.next_batch(100)
	
	print(batch_xs.shape)
	print(batch_ys.shape)
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 784)
	(100, 10)





문제 19. 숫자 2로 채워진 행렬 2x3행렬을 텐서 플로우로 출력하시오 !

	import tensorflow as tf
	
	a = tf.placeholder("float",[2, 3])
	
	sess = tf.Session()
	print(sess.run(a, feed_dict = {a : [[2, 2, 2], [2, 2, 2]]}))

	[[2. 2. 2.]
	 [2. 2. 2.]]



	----아래와 같이 숫자 2를 None으로 바꿔서 실행해 보시오 !
	
	import tensorflow as tf
	
	a = tf.placeholder("float",[None, 3])	# None : 입력되는 행의 갯수가 몇개든 상관 없다는 뜻
	
	sess = tf.Session()
	print(sess.run(a, feed_dict = {a : [[2, 2, 2], [2, 2, 2], [2, 2, 2]]}))

	[[2. 2. 2.]
	 [2. 2. 2.]
	 [2. 2. 2.]]




문제 20. Mnist 데이터 784(28x28)개에 맞게 x 변수를 placeholder로 선언하고 배치로 입력될 데이터의 갯수는 
	 몇개이든 상관없게 None으로 변수를 만들고 Mnist데이터를 x변수에 100개를 담고 출력해 보시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	batch_xs, batch_ys = mnist.test.next_batch(100)
	
	sess = tf.Session()
	print(sess.run(x, feed_dict = {x : batch_xs}).shape)
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 784)



문제 21. 위의 코드를 수정해서 훈련데이터 100개 뿐만 아니라 훈련데이터 라벨 100개도 출력되게끔 코드를 
	 추가하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y = tf.placeholder("float", [None, 10])
	batch_xs, batch_ys = mnist.test.next_batch(100)
	
	sess = tf.Session()
	print(sess.run(x, feed_dict = {x : batch_xs}).shape)
	print(sess.run(y, feed_dict = {y : batch_ys}).shape)
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 784)
	(100, 10)




문제 22. (텐서 플로우로 가중치를 랜점으로 생성하는 방법) 2x3행렬로 -1에서 1사이의 난수를 생성하는 변수 
	 W를 생성하고 안의 내용을 확인하시오 !

	import tensorflow as tf
	
	W = tf.Variable(tf.random_uniform([2,3], -1, 1))
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(W))
	sess.close()

	[[-0.42522287 -0.67010736  0.5873139 ]
	 [ 0.4402938   0.93929124 -0.42909193]]




문제 23. 이번에는 mnist데이터에 맞게 100x784와 내적할 가중치 행렬 784x50으로 W를 생성하시오 !

	import tensorflow as tf
	
	W = tf.Variable(tf.random_uniform([784,50], -1, 1))
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(W))
	sess.close()

	



문제 24. 위에서 만든 입력값 (문제21) 100x784와 지금 만든 가중치 785x50행렬과 내적을 한 결과를 출력하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	W = tf.Variable(tf.random_uniform([784,50], -1, 1))
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	z = tf.matmul(x, W)
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(z, feed_dict = {x : batch_xs}).shape)
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 50)





문제 25. 1x50으로 bias를 생성하는데 변수 b로 생성하고 숫자를 다 1로 채우시오 !

	import tensorflow as tf
	
	b = tf.Variable(tf.ones([50]))
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(b))
	sess.close()
	
	[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
	 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
	 1. 1.]





문제 26. 문제 24번에서 구한 두 행렬의 내적과 지금 방금 생성한 바이어스의 합을 출력하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	W = tf.Variable(tf.random_uniform([784,50], -1, 1))
	b = tf.Variable(tf.ones([50]))
	y = tf.matmul(x, W) + b

	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(y, feed_dict = {x : batch_xs}).shape)
	sess.close()

	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 50)




문제 27. 문제 26번에서 구한 가중의 합인 y값을 시그모이드 함수에 입력해서 출력한 결과를 출력하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	W = tf.Variable(tf.random_uniform([784,50], -1, 1))
	b = tf.Variable(tf.ones([50]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.sigmoid(y)
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(y_hat, feed_dict = {x : batch_xs}).shape)
	sess.close()

	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 50)




문제 28. 위의 활성화 함수를 Relu로 변경하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	W = tf.Variable(tf.random_uniform([784,50], -1, 1))
	b = tf.Variable(tf.ones([50]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.relu(y)
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(y_hat, feed_dict = {x : batch_xs}).shape)
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 50)



문제 29. 이번에는 Relu가 아니라 가중의 합인 y_hat을 softmax 함수를 통과시킨 결과가 어떻게 되는지 확인하시오 !
	 (10개짜리 확률벡터 100개 출력이 예상됨)

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(y_hat, feed_dict = {x : batch_xs}).shape)
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100, 10)




문제 30. 텐서 플로우의 argmax 함수를 이용해서 위에서 출력된 100x10확률벡터들의 최대값의 인덱스 번호를 
	 100개 출력하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(y_predict, feed_dict = {x : batch_xs}))
	sess.close()

	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	[4 2 1 2 2 3 2 1 1 2 2 4 4 2 2 8 2 2 4 2 1 1 2 2 4 4 2 4 2 2 1 1 4 4 1 2 2
	 1 2 6 2 7 1 4 2 2 8 4 2 2 8 4 2 2 1 7 2 4 2 2 1 4 1 4 2 4 1 6 2 6 2 8 1 4
	 2 6 4 4 1 2 2 1 2 2 2 1 4 2 1 2 4 2 2 1 9 2 4 1 2 2]





문제 31. 위의 코드에 라벨을 가져오는 코드를 추가해서 정확도를 출력하시오 !
	 (위의 예상 숫자 100개와 실제 숫자 100개를 비교)

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	y_label = tf.argmax(y_onehot, axis = 1)
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	correction_prediction = tf.equal(y_predict, y_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(accuracy, feed_dict = {x : batch_xs, y_onehot: batch_ys}))
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	0.1









■ Tensorflow로 구현하는 비용함수

	1. 최소 제곱 오차함수 (mean square error)
		loss = tf.square(y_predict, y_label)

	2. 교차 엔트로피 함수 (cross entropy error)
		loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)




문제 32. 문제 31번 코드에 오차함수인 교차 엔트로피 함수를 추가하고 loss를 출력하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	# y_label = tf.argmax(y_onehot, axis = 1)
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	# correction_prediction = tf.equal(y_predict, y_label)
	# accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)
	
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	print(sess.run(loss, feed_dict = {x : batch_xs, y_onehot: batch_ys}).shape)
	sess.close()

	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	(100,)








■ 경사 감소법을 Tensorflow로 구현하는 방법

	############################################
	
	# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
	 
		SGD : 
		 미니배치만큼 랜덤으로 데이터를 추출해서 확률적으로 경사를 감소하여
		 global minima 로 찾아가는 방법 
		
		단점 :  Local minima 에 잘 빠진다. 
	

	# optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)
	
		러닝 레이트가 학습되면서 자동 조절되는 경사감소법 
	

	# optimizer = tf.train.MomentumOptimizer(learning_rate=0.01)
	
		관성을 이용해서 local minima 에 안빠지게 하는 경사감소법 
	

	# optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
	
		Adagrade 의 장점 + Momentum 의 장점 
	
	############################################




문제 33. 지금까지 만든 코드에 Adam 경사감소법 코드를 추가해서 학습이 되게 하시오 1

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	y_label = tf.argmax(y_onehot, axis = 1)
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	correction_prediction = tf.equal(y_predict, y_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
	train = optimizer.minimize(loss)
	
	batch_xs, batch_ys = mnist.test.next_batch(100)
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	sess.run(train, feed_dict = {x : batch_xs, y_onehot: batch_ys})
	print(sess.run(accuracy, feed_dict = {x : batch_xs, y_onehot : batch_ys}))
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	
	0.09





문제 34. 위의 코드에 for loop문을 이용해서 1에폭 돌게 구성하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	y_label = tf.argmax(y_onehot, axis = 1)
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	correction_prediction = tf.equal(y_predict, y_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 601):
	    batch_xs, batch_ys = mnist.test.next_batch(100)
	    sess.run(train, feed_dict = {x : batch_xs, y_onehot: batch_ys})
	    print(sess.run(accuracy, feed_dict = {x : batch_xs, y_onehot : batch_ys}))
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	0.96





문제 35. 위의 코드를 수정해서 아래와 같이 결과가 출력되게 하시오 ! (오늘의 마지막 문제)

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	y_label = tf.argmax(y_onehot, axis = 1)
	W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	correction_prediction = tf.equal(y_predict, y_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 4):
	    for j in range(1, 601):
	        batch_xs, batch_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : batch_xs, y_onehot: batch_ys})
	    print('%d 에폭 정확도'%i, sess.run(accuracy, feed_dict = {x : batch_xs, y_onehot : batch_ys}))
	
	sess.close()
	
	Extracting MNIST_data/train-images-idx3-ubyte.gz
	Extracting MNIST_data/train-labels-idx1-ubyte.gz
	Extracting MNIST_data/t10k-images-idx3-ubyte.gz
	Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
	1 에폭 정확도 0.95
	2 에폭 정확도 0.98
	3 에폭 정확도 0.96