■ 4. 텐서 플로우로 다층 신경망 구성

	* Underfitting을 막을 수 있는 방법
		1. 가중치 초기화
		2. 배치 정규화

	* Overfitting을 막을 수 있는 방법
		1. 드롭아웃
		








★ 텐서 플로우에서 가중치 초기화 하는 방법

	1. Xavier ─▶ 1 / √n
	2. He     ─▶ √2/n


############################################
# 가중치 초기화 방법
# W = tf.Variable(tf.random_uniform([784,10], -1, 1)) 
# W = tf.get_variable(name="W", shape=[784, 10], initializer=tf.contrib.layers.xavier_initializer()) 
# xavier 초기값
# W = tf.get_variable(name='W', shape=[784, 10], initializer=tf.contrib.layers.variance_scaling_initializer()) 
# he 초기값
# b = tf.Variable(tf.zeros([10]))
############################################




문제 36. 문제 35번까지 만든 단층 신경망에 가중치 초기화를 he를 달아서 학습 시키시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	y_label = tf.argmax(y_onehot, axis = 1)
	# W = tf.Variable(tf.random_uniform([784,10], -1, 1))
	# xavier 초기값
	# W = tf.get_variable(name="W", shape=[784, 10], initializer=tf.contrib.layers.xavier_initializer()) 
	# he 초기값
	W = tf.get_variable(name='W', shape=[784, 10], \
			    initializer=tf.contrib.layers.variance_scaling_initializer()) 
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	correction_prediction = tf.equal(y_predict, y_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 4):
	    for j in range(1, 601):
	        batch_xs, batch_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : batch_xs, y_onehot: batch_ys})
	    print('%d 에폭 정확도'%i, sess.run(accuracy, feed_dict = {x : batch_xs, y_onehot : batch_ys}))

	sess.close()


	* 텐서 플로우에서 가중치 초기화 할때 주의사항 !!
		주피터 노트북과 스파이더는 오류가 나므로 tf.reset_default_graph()를 맨 위에다가
		적어줘야 한다.

		텐서 플로우는 그래프를 메모리에 올려서 실행하게 되는데 파이참은 코드를 매번 실행할때마다
		메모리를 지워주는데 스파이더나 주피터는 대화형이라서 실행할때 마다 메모리에 그래프가 
		누적이 되어서 맨위에 tf.reset_default_graph()를 맨 위에다가 적어줘야 한다.




문제 37. learning rate를 0.05로 해서 다시 학습 시키시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	y_onehot = tf.placeholder("float", [None, 10])
	y_label = tf.argmax(y_onehot, axis = 1)
	W = tf.get_variable(name='W', shape=[784, 10], \
			    initializer=tf.contrib.layers.variance_scaling_initializer())
	b = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W) + b
	y_hat = tf.nn.softmax(y)
	y_predict = tf.argmax(y_hat, axis = 1)
	correction_prediction = tf.equal(y_predict, y_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(y_onehot*tf.log(y_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.05)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 20):
	    for j in range(1, 601):
	        batch_xs, batch_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : batch_xs, y_onehot: batch_ys})
	    print('%d 에폭 정확도'%i, sess.run(accuracy, feed_dict = {x : batch_xs, y_onehot : batch_ys}))
	
	sess.close()





문제 38. 위의 단층 신경망을 다층(2층) 신경망으로 변환해서 돌리시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	W1 = tf.get_variable(name='W1', shape=[784, 50], \
			     initializer=tf.contrib.layers.variance_scaling_initializer())
	b1 = tf.Variable(tf.ones([50]))
	W2 = tf.get_variable(name='W2', shape=[50, 10], \
			     initializer=tf.contrib.layers.variance_scaling_initializer())
	b2 = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W1) + b1
	y_hat = tf.nn.relu(y)
	z = tf.matmul(y_hat, W2) + b2
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0008)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 20):
	    for j in range(1, 601):
	        batch_xs, batch_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : batch_xs, z_onehot: batch_ys})
	    print('%d 에폭 정확도'%i, sess.run(accuracy, feed_dict = {x : batch_xs, z_onehot : batch_ys}))
	
	sess.close()















★ 텐서플로우로 배치정규화 구현

	배치정규화 ? 
		신경망 학습시 가중치의 값의 데이터가 골고루 분산될 수 있도록 하는 것을 강제하는 장치
	
	구현 코드 ? 
		batch_z1 = tf.contrib.layers.batch_norm(z1,True)




문제 39. 문제 38번에서 완성한 3층 신경망에 배치 정규화 코드를 추가해서 돌리고 정확도를 확인하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	W1 = tf.get_variable(name='W1', shape=[784, 50], \
	initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b1 = tf.Variable(tf.ones([50]))
	W2 = tf.get_variable(name='W2', shape=[50, 10], \
	initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b2 = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W1) + b1
	batch_y = tf.contrib.layers.batch_norm(y,True)
	y_hat = tf.nn.relu(y)
	z = tf.matmul(y_hat, W2) + b2
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 15):
	    for j in range(1, 601):
	        batch_xs, batch_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : batch_xs, z_onehot: batch_ys})
	    print('%d 에폭 정확도'%i, sess.run(accuracy, feed_dict = {x : batch_xs, z_onehot : batch_ys}))
	
	sess.close()










★ 훈련하는 신경망에 테스트를 하는 코드를 추가




문제 40. 지금 현재까지의 코드는 신경망을 훈련만 시키는 코드였는데 테스트 데이터도 신경망에 입력해서 
	 오버피팅이 발생하는지 확인할 수 있도록 훈련 데이터의 정확도와 테스트 데이터의 정확도를 같이 
	 출력할 수 있도록 코드를 작성하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	
	W1 = tf.get_variable(name='W1', shape=[784, 50], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b1 = tf.Variable(tf.ones([50]))
	W2 = tf.get_variable(name='W2', shape=[50, 10], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b2 = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W1) + b1
	batch_y = tf.contrib.layers.batch_norm(y,True)
	y_hat = tf.nn.relu(y)
	z = tf.matmul(y_hat, W2) + b2
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 15):
	    for j in range(1, 601):
	        train_xs, train_ys = mnist.train.next_batch(100)
	        test_xs, test_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : train_xs, z_onehot: train_ys})
	    print('train %d 에폭 정확도'%i, \
	          sess.run(accuracy, feed_dict = {x : train_xs, z_onehot : train_ys}))
	    print('test  %d 에폭 정확도'%i, \
	          sess.run(accuracy, feed_dict = {x : test_xs, z_onehot : test_ys}))
	    print('========================')

sess.close()





문제 41. 위의 결과를 그래프로 시각화 하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	import matplotlib.pyplot as plt
	import numpy as np
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	
	W1 = tf.get_variable(name='W1', shape=[784, 50], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b1 = tf.Variable(tf.ones([50]))
	W2 = tf.get_variable(name='W2', shape=[50, 10], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b2 = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W1) + b1
	batch_y = tf.contrib.layers.batch_norm(y,True)
	y_hat = tf.nn.relu(y)
	z = tf.matmul(y_hat, W2) + b2
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	train_acc_list = []
	test_acc_list = []
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 15):
	    for j in range(1, 601):
	        train_xs, train_ys = mnist.train.next_batch(100)
	        test_xs, test_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : train_xs, z_onehot: train_ys})
	    train_acc = sess.run(accuracy, feed_dict = {x : train_xs, z_onehot : train_ys})
	    test_acc = sess.run(accuracy, feed_dict = {x : test_xs, z_onehot : test_ys})
	    print('train %d 에폭 정확도'%i, train_acc)
	    train_acc_list.append(train_acc)
	    print('test  %d 에폭 정확도'%i, test_acc)
	    test_acc_list.append(test_acc)
	    print('========================')
	
	sess.close()
	
	# 그래프 그리기
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()








★ 오버피팅이 발생하지 않도록 drop out을 적용하는 방법




문제 42. 책 132페이지를 참고해서 훈련을 다 시키고 나서 테스트 데이터를 입려갛도록 코드를 
	 작성하시오 ! (점심시간 문제)
	 드롭아웃은 훈련할때는 50%의 노드로 학습하고 테스트 할때는 100%의 노드로 테스트하게 하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	import matplotlib.pyplot as plt
	import numpy as np
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	keep_prob = tf.placeholder("float")
	
	W1 = tf.get_variable(name='W1', shape=[784, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b1 = tf.Variable(tf.ones([100]))
	W2 = tf.get_variable(name='W2', shape=[100, 10], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b2 = tf.Variable(tf.ones([10]))
	y = tf.matmul(x, W1) + b1
	batch_y = tf.contrib.layers.batch_norm(y,True)
	y_hat = tf.nn.relu(y)
	y_drop = tf.nn.dropout(y_hat, keep_prob)
	z = tf.matmul(y_drop, W2) + b2
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	train_acc_list = []
	test_acc_list = []
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 15):
	    for j in range(1, 601):
	        train_xs, train_ys = mnist.train.next_batch(100)
	        test_xs, test_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : train_xs, z_onehot: train_ys, keep_prob : 0.5})
	    train_acc = sess.run(accuracy, feed_dict = {x : train_xs, z_onehot : train_ys, keep_prob : 1.0})
	    print('train %d 에폭 정확도'%i, train_acc)
	    train_acc_list.append(train_acc)
	    test_acc = sess.run(accuracy, feed_dict = {x : test_xs, z_onehot : test_ys, keep_prob : 1.0})
	    print('test  %d 에폭 정확도'%i, test_acc)
	    test_acc_list.append(test_acc)
	    print('=========================')
	
	sess.close()
	
	# 그래프 그리기
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()





문제 43. 2층 신경망 ──▶ 3층 신경망으로 변경하고 정확도를 확인하시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	import matplotlib.pyplot as plt
	import numpy as np
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x = tf.placeholder("float", [None,784])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	keep_prob = tf.placeholder("float")
	
	W1 = tf.get_variable(name='W1', shape=[784, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b1 = tf.Variable(tf.ones([100]))
	W2 = tf.get_variable(name='W2', shape=[100, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b2 = tf.Variable(tf.ones([100]))
	
	W3 = tf.get_variable(name='W3', shape=[100, 10], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer()) # he 초기값
	b3 = tf.Variable(tf.ones([10]))
	
	y = tf.matmul(x, W1) + b1
	batch_y = tf.contrib.layers.batch_norm(y,True)
	y_hat = tf.nn.relu(y)
	y_drop = tf.nn.dropout(y_hat, keep_prob)
	y1 = tf.matmul(y_drop, W2) + b2
	y1_hat = tf.nn.relu(y1)
	y1_drop = tf.nn.dropout(y1_hat, keep_prob)
	z = tf.matmul(y1_drop, W3) + b3
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	train_acc_list = []
	test_acc_list = []
	
	sess = tf.Session()
	sess.run(init)
	for i in range(1, 15):
	    for j in range(1, 601):
	        train_xs, train_ys = mnist.train.next_batch(100)
	        test_xs, test_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x : train_xs, z_onehot: train_ys, keep_prob : 0.9})
	    train_acc = sess.run(accuracy, feed_dict = {x : train_xs, z_onehot : train_ys, keep_prob : 1.0})
	    print('train %d 에폭 정확도'%i, train_acc)
	    train_acc_list.append(train_acc)
	    test_acc = sess.run(accuracy, feed_dict = {x : test_xs, z_onehot : test_ys, keep_prob : 1.0})
	    print('test  %d 에폭 정확도'%i, test_acc)
	    test_acc_list.append(test_acc)
	    print('=========================')
	
	sess.close()
	
	# 그래프 그리기
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()









■ 텐서 플로우로 CNN 구현하기

	입력층  ──▶  은닉 1층  ──▶  은닉2층  ──▶  은닉 3층  ──▶  출력층(4층)
	28, 28, 1	(conv-pool)	    100 	     100 		 10





문제 44. 텐서플로우로 CNN구현하기

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	import matplotlib.pyplot as plt
	import numpy as np
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	tf.reset_default_graph()
	x_load = tf.placeholder("float", [None, 784])
	
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	keep_prob = tf.placeholder("float")
	
	W = tf.get_variable(name='W', shape=[3, 3, 1, 32], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b = tf.Variable(tf.ones([28, 28, 32]))
	W1 = tf.get_variable(name='W1', shape=[6272, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b1 = tf.Variable(tf.ones([100]))
	W2 = tf.get_variable(name='W2', shape=[100, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b2 = tf.Variable(tf.ones([100]))
	W3 = tf.get_variable(name='W3', shape=[100, 10], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b3 = tf.Variable(tf.ones([10]))
	x = tf.reshape(x_load, [-1 ,28, 28, 1])
	x1 = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = 'SAME')
	x1_hat = tf.nn.relu(x1) + b
	x1_pool = tf.nn.max_pool(x1_hat, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
	x1_res = tf.reshape(x1_pool, [-1, 6272])
	y = tf.matmul(x1_res, W1) + b1
	batch_y = tf.contrib.layers.batch_norm(y,True)
	y_hat = tf.nn.relu(y)
	y_drop = tf.nn.dropout(y_hat, keep_prob)
	y1 = tf.matmul(y_drop, W2) + b2
	y1_hat = tf.nn.relu(y1)
	y1_drop = tf.nn.dropout(y1_hat, keep_prob)
	z = tf.matmul(y1_drop, W3) + b3
	z_hat = tf.nn.softmax(z)
	z_predict = tf.argmax(z_hat, axis = 1)
	
	correction_prediction = tf.equal(z_predict, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	train_acc_list = []
	test_acc_list = []
	
	sess = tf.Session()
	sess.run(init)
	
	train_xs, train_ys = mnist.train.next_batch(100)
	test_xs, test_ys = mnist.test.next_batch(100)
	train_acc = sess.run(accuracy, \
	                     feed_dict = {x_load : train_xs, z_onehot : train_ys, keep_prob : 1.0})
	print('train   초기 정확도', train_acc)
	train_acc_list.append(train_acc)
	test_acc = sess.run(accuracy, \
	                    feed_dict = {x_load : test_xs, z_onehot : test_ys, keep_prob : 1.0})
	print('test    초기 정확도', test_acc)
	test_acc_list.append(test_acc)
	print('=========================')
	for i in range(1, 16):
	    for j in range(1, 601):
	        train_xs, train_ys = mnist.train.next_batch(100)
	        test_xs, test_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x_load : train_xs, z_onehot: train_ys, keep_prob : 0.9})
	    train_acc = sess.run(accuracy, \
	                         feed_dict = {x_load : train_xs, z_onehot : train_ys, keep_prob : 1.0})
	    print('train %d 에폭 정확도'%i, train_acc)
	    train_acc_list.append(train_acc)
	    test_acc = sess.run(accuracy, \
	                        feed_dict = {x_load : test_xs, z_onehot : test_ys, keep_prob : 1.0})
	    print('test  %d 에폭 정확도'%i, test_acc)
	    test_acc_list.append(test_acc)
	    print('=========================')
	
	sess.close()
	
	# 그래프 그리기
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()





문제 44. 문제 43번 4층 신경망에 conv-poling 층을 하나 더 추가해서 5층으로 변경을하고 gpu에서 돌리시오 !

	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	import matplotlib.pyplot as plt
	import numpy as np
	
	mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
	
	
	# 입력층
	tf.reset_default_graph()
	x_load = tf.placeholder("float", [None, 784])
	x = tf.reshape(x_load, [-1 ,28, 28, 1])
	z_onehot = tf.placeholder("float", [None, 10])
	z_label = tf.argmax(z_onehot, axis = 1)
	keep_prob = tf.placeholder("float")
	
	# 은닉 1층
	W1 = tf.get_variable(name='W1', shape=[3, 3, 1, 32], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b1 = tf.Variable(tf.ones([28, 28, 32]))
	
	x = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding = 'SAME')
	x = tf.nn.relu(x) + b1
	x = tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
	# x = tf.reshape(x, [-1, 6272])
	
	
	
	# 은닉 2층
	W2 = tf.get_variable(name='W2', shape=[3, 3, 32, 64], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b2 = tf.Variable(tf.ones([14, 14, 64]))
	
	x1 = tf.nn.conv2d(x, W2, strides=[1, 1, 1, 1], padding = 'SAME')
	x1 = tf.nn.relu(x1) + b2
	x1 = tf.nn.max_pool(x1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')
	x1 = tf.reshape(x1, [-1, 3136])
	
	
	
	# 은닉 3층
	W3 = tf.get_variable(name='W3', shape=[3136, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b3 = tf.Variable(tf.ones([100]))
	
	y = tf.matmul(x1, W3) + b3
	y = tf.contrib.layers.batch_norm(y,True)
	y = tf.nn.relu(y)
	y = tf.nn.dropout(y, keep_prob)
	
	
	
	# 은닉 3층
	W4 = tf.get_variable(name='W4', shape=[100, 100], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b4 = tf.Variable(tf.ones([100]))
	
	y1 = tf.matmul(y, W4) + b4
	y1 = tf.nn.relu(y1)
	y1 = tf.nn.dropout(y1, keep_prob)
	
	
	
	# 출력층(4층)
	W5 = tf.get_variable(name='W5', shape=[100, 10], \
	                     initializer=tf.contrib.layers.variance_scaling_initializer())
	b5 = tf.Variable(tf.ones([10]))
	
	z = tf.matmul(y1, W5) + b5
	z_hat = tf.nn.softmax(z)
	z = tf.argmax(z_hat, axis = 1)


	
	correction_prediction = tf.equal(z, z_label)
	accuracy = tf.reduce_mean(tf.cast(correction_prediction, "float"))
	loss = - tf.reduce_sum(z_onehot*tf.log(z_hat), axis = 1)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
	train = optimizer.minimize(loss)
	
	init = tf.global_variables_initializer()
	train_acc_list = []
	test_acc_list = []
	
	
	# 세션 활성화
	sess = tf.Session()
	sess.run(init)
	
	train_xs, train_ys = mnist.train.next_batch(100)
	test_xs, test_ys = mnist.test.next_batch(100)
	train_acc = sess.run(accuracy, \
	                     feed_dict = {x_load : train_xs, z_onehot : train_ys, keep_prob : 1.0})
	print('train   초기 정확도', train_acc)
	train_acc_list.append(train_acc)
	test_acc = sess.run(accuracy, \
	                    feed_dict = {x_load : test_xs, z_onehot : test_ys, keep_prob : 1.0})
	print('test    초기 정확도', test_acc)
	test_acc_list.append(test_acc)
	print('=========================')
	for i in range(1, 16):
	    for j in range(1, 601):
	        train_xs, train_ys = mnist.train.next_batch(100)
	        test_xs, test_ys = mnist.test.next_batch(100)
	        sess.run(train, feed_dict = {x_load : train_xs, z_onehot: train_ys, keep_prob : 0.9})
	    train_acc = sess.run(accuracy, \
	                         feed_dict = {x_load : train_xs, z_onehot : train_ys, keep_prob : 1.0})
	    print('train %d 에폭 정확도'%i, train_acc)
	    train_acc_list.append(train_acc)
	    test_acc = sess.run(accuracy, \
	                        feed_dict = {x_load : test_xs, z_onehot : test_ys, keep_prob : 1.0})
	    print('test  %d 에폭 정확도'%i, test_acc)
	    test_acc_list.append(test_acc)
	    print('=========================')
	
	sess.close()
	
	
	
	# 그래프 그리기
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()











■ cifar10 데이터를 신경망에 로드하는 데이터 전처리 코드 작성

	* cifar10 데이터 소개
		cifar10은 총 60000개의 데이터 셋으로 이루어져 있으며 그 중 50000개 훈련데이터 이고
		10000개가 테스트 데이터이다.

		class는 비행기부터 트럭까지 10개로 구성되어 있다.

		1.  비행기
		2.  자동차
		3.  새
		4.  고양이
		5.  사슴
		6.  개
		7.  개구리
		8.  말
		9.  양
		10. 트럭




문제 45. c:\a\cifar10\test 폴더를 만들고 test이미지 100개를 이 폴더에 따로 복사하고 복사한 이미지를 
	 아래와 같이 불러오는 함수를 생성하시오 !

	import os
	
	test_image = "c:\\a\\cifar10\\test"
	
	def image_load(path):
	    file_list = os.listdir(path)
	    return file_list

	print(image_load(test_image))

	['1.png', '10.png', '100.png', '11.png', '12.png', ....... '98.png', '99.png']



문제 46. 위의 함수를 수정해서 아래와 같이 숫자만 출력되게 하시오 !

	import os
	
	test_image = "c:\\a\\cifar10\\test"
	
	def image_load(path):
	    file_list = os.listdir(path)
	    for i in range(len(file_list)):
	        file_list[i] = int(file_list[i][:-4])
	    file_list.sort()
	    return file_list
	    
	print(image_load(test_image))

	[1, 10, 100, 11, 12, 13, ......   95, 96, 97, 98, 99]





문제 47. 아래의 결과를 정렬해서 출력되게 하시오 !

	import os
	
	test_image = "c:\\a\\cifar10\\test"
	
	def image_load(path):
	    file_list = os.listdir(path)
	    for i in range(len(file_list)):
	        file_list[i] = int(file_list[i][:-4])
	    file_list.sort()
	    return file_list
	    
	print(image_load(test_image))

	[1, 2, 3, 4, 5, 6, 7, 8, 9,    ......     95, 96, 97, 98, 99, 100]





문제 48. 문제 47번에 나온 결과에 png를 붙여서 아래와 같이 결과가 출력되게 하시오 !

	import os
	
	test_image = "c:\\a\\cifar10\\test"
	
	def image_load(path):
	    file_list = os.listdir(path)
	    for i in range(len(file_list)):
	        file_list[i] = int(file_list[i][:-4])
	    file_list.sort()
	    for i in range(len(file_list)):
	        file_list[i] = str(file_list[i]) + ".png"
	    return file_list
	    
	print(image_load(test_image))
	
	['1.png', '2.png', '3.png', '4.png', '5.png', ......  '98.png', '99.png', '100.png']





문제 49. 이미지 이름 앞에 절대경로가 아래처럼 붙게 하시오 !

	import os
	
	test_image = "c:\\a\\cifar10\\test"
	
	def image_load(path):
	    file_list = os.listdir(path)
	    for i in range(len(file_list)):
	        file_list[i] = int(file_list[i][:-4])
	    file_list.sort()
	    for i in range(len(file_list)):
	        file_list[i] = path + "\\" + str(file_list[i]) + ".png"
	    return file_list
	    
	print(image_load(test_image))

	['c:\\a\\cifar10\\test\\1.png',     .....     'c:\\a\\cifar10\\test\\100.png']





